{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4ffc67",
   "metadata": {},
   "source": [
    "# Creating the daily graph batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c38d8440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(120000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "%autosave 120\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import utils as ut\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec22638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN related imports\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.data as data\n",
    "import torch_geometric.transforms as transforms\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import InMemoryDataset, Data, download_url, extract_zip, HeteroData\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import minmax_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537aee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs folder created\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def walk_up_folder(path, depth=1):\n",
    "    \"\"\"\n",
    "    Helper method to navigate the file system and get to the file location\n",
    "    \"\"\"\n",
    "    _cur_depth = 1\n",
    "    while _cur_depth < depth:\n",
    "        path = os.path.dirname(path)\n",
    "        _cur_depth += 1\n",
    "    return path\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "log_dir = os.path.join(walk_up_folder(os.getcwd(), 3), \"dataset/logs/\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    print(\"Logs folder created\")\n",
    "\n",
    "\n",
    "raw_dir = os.path.join(walk_up_folder(os.getcwd(), 3), \"dataset/root/raw/\")\n",
    "proc_dir = os.path.join(walk_up_folder(os.getcwd(), 3), \"dataset/root/processed/\")\n",
    "mapping_dir = os.path.join(walk_up_folder(os.getcwd(), 3), \"dataset/root/mapping/\")\n",
    "split_dir = os.path.join(walk_up_folder(os.getcwd(), 3), \"dataset/root/split/\")\n",
    "node_dir = os.path.join(raw_dir, \"node-feat\")\n",
    "edge_dir = os.path.join(raw_dir, \"relations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f53af",
   "metadata": {},
   "source": [
    "## Creating out own inmemory dataset\n",
    "\n",
    "You can find graph data to test from \n",
    "\n",
    "* https://networkrepository.com/econ.php \n",
    "* https://networkrepository.com/dynamic.php \n",
    "* https://networkrepository.com/heter.php\n",
    "* https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html?highlight=OGB_MAG#torch_geometric.datasets.OGB_MAG  \n",
    "* https://github.com/Soughing0823/LAGNN/blob/main/utils.py \n",
    "* \n",
    "\n",
    "From the official PyG documentation we see that some methods need to be override:\n",
    "\n",
    "1) torch_geometric.data.InMemoryDataset.`raw_file_names()`: A list of files in the raw_dir which needs to be found in order to skip the download.\n",
    "\n",
    "2) torch_geometric.data.InMemoryDataset.`processed_file_names()`: A list of files in the processed_dir which needs to be found in order to skip the processing.\n",
    "\n",
    "3) torch_geometric.data.InMemoryDataset.`download()`: Downloads raw data into raw_dir.\n",
    "\n",
    "4) torch_geometric.data.InMemoryDataset.`process()`: Processes raw data and saves it into the processed_dir.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21f735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= os.path.join(ut.walk_up_folder(os.getcwd(), 5), \"wmt-mobius-dev-svcmobius.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e48df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501b472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a43028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFolder = osp.join(ut.walk_up_folder(os.getcwd(),5),'dataset_old')\n",
    "fileset = set()\n",
    "for root, dirs, files in os.walk(myFolder):\n",
    "    for fileName in files:\n",
    "        if fileName=='.DS_Store': continue\n",
    "        fileset.add( os.path.join( root[len(myFolder):], fileName )[1:])\n",
    "\n",
    "\n",
    "\n",
    "# ut.gcp_df_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36fd9cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'root/raw/relations/2021-02-24.csv'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52bfda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in uploaded:\n",
    "    try: \n",
    "        fileset.remove(file)\n",
    "    except:\n",
    "        x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b97b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in fileset: print(f)\n",
    "# fileset.remove('root/processed/data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22175b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-23.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-15.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-24.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-19.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-25.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-06.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-20.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-08.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-05.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/mapping/product_entidx2name.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-11.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-11.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-11.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-02.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-16.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-29.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-19.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-05.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-15.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-20.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-23.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-28.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-20.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-31.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-13.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-28.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-11.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-26.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-26.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-16.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-28.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-18.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-19.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-23.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-03.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-12.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-19.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-12.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-25.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/mapping/shipnode_entidx2name.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-20.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-17.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-24.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-18.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-13.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-15.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-17.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-26.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-06.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-15.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-16.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-06.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-08.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-19.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-20.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-18.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-27.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-01.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-16.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-24.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-03.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-26.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-05.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-03.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-13.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-16.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-11.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-08.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-09.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-01.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-23.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-30.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-20.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-27.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-03.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-18.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-03.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-18.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-19.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-25.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/processed/pre_transform.pt has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-05.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-18.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-09.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-02.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-12.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/processed/pre_filter.pt has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-26.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-12.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-13.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-26.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-19.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-25.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-08.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-29.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-05.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-30.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-12.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-25.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-01.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-09.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-17.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-08.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-09.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-28.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-27.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-16.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-24.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-12.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-06.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-23.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-01-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-06.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-13.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-25.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-28.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-11.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-28.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-05.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-07.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-01.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-06.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-02.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-20.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-03.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-26.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-24.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-23.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-30.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-13.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-25.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-17.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-21.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-15.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-31.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-17.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-02.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-13.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-10.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-10-26.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-14.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-23.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-03.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-08-24.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-02.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-29.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-02-22.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-17.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-05.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-16.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-06-08.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-12-29.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-01.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-11-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-25.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-03-29.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-11.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-07-04.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-09-28.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-04-23.csv has been uploaded\n",
      "CSV file - /Users/a0s0u22/Mobius/dataset_old/root/raw/relations/2021-05-15.csv has been uploaded\n"
     ]
    }
   ],
   "source": [
    "for file in fileset:\n",
    "    # if file in uploaded: continue\n",
    "    source = os.path.join(myFolder,file)\n",
    "    destination =  os.path.join('n0s011m/dataset_s2h_and_mp_30_subcats',file)\n",
    "    ut.gcp_df_to_csv('mobius_data_science',source,destination)\n",
    "    uploaded.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf297cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityEncoder(object):\n",
    "    def __init__(self, dtype=None):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, df):\n",
    "        return torch.from_numpy(df.values).view(-1, 1).to(self.dtype)\n",
    "\n",
    "\n",
    "class WmtNetworkDataset(InMemoryDataset):\n",
    "    def __init__(self, root_dir, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data).\n",
    "        \"\"\"\n",
    "        self.root = root_dir\n",
    "        super(WmtNetworkDataset, self).__init__(root_dir, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return osp.join(self.root, 'raw/relations')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return osp.join(self.root, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # edge relations files\n",
    "        file_list = sorted(os.listdir(self.raw_dir))\n",
    "        return [osp.join(self.raw_dir, f) for f in file_list]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        ...\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        proc_dir = osp.join(self.root, \"processed/\")\n",
    "        mapping_dir = osp.join(self.root, \"mapping/\")\n",
    "        split_dir = osp.join(self.root, \"split/\")\n",
    "\n",
    "        node_dir = osp.join(ut.walk_up_folder(self.raw_dir, 2), \"node-feat\")\n",
    "        edge_dir = self.raw_dir\n",
    "\n",
    "        # File locations for the mappings and the features of the nodes\n",
    "        cust_mapping_path = os.path.join(mapping_dir, 'cust_entidx2name.csv')\n",
    "        ship_mapping_path = os.path.join(mapping_dir, 'shipnode_entidx2name.csv')\n",
    "        prod_mapping_path = os.path.join(mapping_dir, 'product_entidx2name.csv')\n",
    "\n",
    "        cust_node_feats = os.path.join(node_dir, 'custnode-feat.csv')\n",
    "        ship_node_feats = os.path.join(node_dir, 'shipnode-feat.csv')\n",
    "        prod_node_feats = os.path.join(node_dir, 'productnode-feat.csv')\n",
    "\n",
    "        # Get node features\n",
    "        # Loading Node mappings and their features\n",
    "        cust_x, cust_idx_mapping = self.load_node_csv(cust_node_feats, cust_mapping_path)\n",
    "        ship_x, ship_idx_mapping = self.load_node_csv(ship_node_feats, ship_mapping_path)\n",
    "        prod_x, prod_idx_mapping = self.load_node_csv(prod_node_feats, prod_mapping_path)\n",
    "\n",
    "        # Get edge features\n",
    "        # In the loop we extract the nodes' embeddings, edges connectivity for\n",
    "        # and label for a graph, process the information and put it in a Data\n",
    "        # object, then we add the object to a list\n",
    "        sdc_edge_indices, sdc_edge_attrs, sdc_edge_labels, cop_edge_indices, cop_edge_attrs, cop_edge_labels = self.get_edges_from_files(\n",
    "            cust_idx_mapping, ship_idx_mapping, prod_idx_mapping)\n",
    "\n",
    "        # Create data object\n",
    "\n",
    "        data = HeteroData()\n",
    "        data['product'].x = prod_x  # [num_products, num_features_product]\n",
    "        data['customer'].x = cust_x  # [num_customers, num_features_customer]\n",
    "        data['shipnode'].x = ship_x  # [num_shipnodes, num_features_shipnode]\n",
    "\n",
    "        for node_type in ['customer', 'product', 'shipnode']:\n",
    "            data[node_type].num_nodes = data[node_type].x.size(0)\n",
    "\n",
    "        for edge_type in [('customer', 'orders', 'product'), ('shipnode', 'delivers', 'customer')]:\n",
    "            if edge_type == ('customer', 'orders', 'product'):\n",
    "                data['customer', 'orders', 'product'].edge_index = torch.cat(cop_edge_indices,\n",
    "                                                                             1)  # [2, num_edges_orders]\n",
    "                data['customer', 'orders', 'product'].edge_attr = minmax_scale(torch.cat(cop_edge_attrs,\n",
    "                                                                            0))  # [num_edges_orders, num_features_orders]\n",
    "            else:\n",
    "                data['shipnode', 'delivers', 'customer'].edge_index = torch.cat(sdc_edge_indices,\n",
    "                                                                                1)  # [2, num_edges_delivered]\n",
    "                data['shipnode', 'delivers', 'customer'].edge_attr = minmax_scale(torch.cat(sdc_edge_attrs,\n",
    "                                                                               0))  # [num_edges_affiliated, num_features_affiliated]\n",
    "                data['shipnode', 'delivers', 'customer'].edge_label = torch.cat(sdc_edge_labels, 0)  # [num_edges,1]\n",
    "\n",
    "        # Apply the functions specified in pre_filter and pre_transform\n",
    "        if self.pre_filter is not None:\n",
    "            data = self.pre_filter(data)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        # Store the processed data\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def load_node_csv(self, featpath, idxpath):\n",
    "        \"\"\"\n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(featpath)\n",
    "        map_df = pd.read_csv(idxpath)\n",
    "        mapping = dict(zip(map_df[\"ent_name\"], map_df[\"ent_idx\"]))\n",
    "        x = torch.tensor(df.values, dtype=torch.float)\n",
    "        return x, mapping\n",
    "\n",
    "    def load_edge_csv(self, edge_file_path, edge_cols, src_index_col, src_mapping, dst_index_col, dst_mapping,\n",
    "                      encoders=None):\n",
    "        \"\"\"\n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of edges, Edge Feature size]\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(edge_file_path)\n",
    "        # src = [src_mapping[index] for index in df[src_index_col]]\n",
    "        # dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
    "        src = []\n",
    "        dst = []\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                s = src_mapping[row[src_index_col]]\n",
    "                d = dst_mapping[row[dst_index_col]]\n",
    "            except:\n",
    "                df.drop(index, inplace=True)\n",
    "                # print(\"Missed a key\")\n",
    "                continue\n",
    "            src.append(s)\n",
    "            dst.append(d)\n",
    "        edge_index = torch.tensor([src, dst])\n",
    "\n",
    "        edge_attr = df[edge_cols]\n",
    "        edge_attr = torch.tensor(edge_attr.values, dtype=torch.float)\n",
    "\n",
    "        edge_label = None\n",
    "        if encoders is not None:\n",
    "            edge_label = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "            edge_label = torch.cat(edge_label, dim=-1)\n",
    "\n",
    "        return edge_index, edge_attr, edge_label\n",
    "\n",
    "    def get_edges_from_files(self, cust_idx_mapping, ship_idx_mapping, prod_idx_mapping):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Between customer and shipnode - involves delivery distance and how iot is shipped\n",
    "        # cust_order_type\n",
    "\n",
    "        dlvr_edge_attrb = ['same_zip', 'edge_travel_dist', 'shpg_cost_amt', 'inbound_shpg_cost_actl_amt']\n",
    "\n",
    "        # Between shipnode and product - involves item quantity and shipping costs\n",
    "        prod_edge_attrb = ['prod_cost_plcd_amt', 'net_qty', 'plcd_orig_qty', 'tax_plcd_amt', 'fulfmt_cost_amt',\n",
    "                           'gmv_plcd_amt']\n",
    "\n",
    "        time_attrb = ['order_hour', 'order_minute', 'order_plcd_tsYear', 'order_plcd_tsMonth',\n",
    "                      'order_plcd_tsWeek', 'order_plcd_tsDay', 'order_plcd_tsDayofweek',\n",
    "                      'order_plcd_tsDayofyear', 'order_plcd_tsIs_month_end',\n",
    "                      'order_plcd_tsIs_month_start', 'order_plcd_tsIs_quarter_end',\n",
    "                      'order_plcd_tsIs_quarter_start', 'order_plcd_tsIs_year_end',\n",
    "                      'order_plcd_tsIs_year_start']\n",
    "\n",
    "        label = ['SLA']\n",
    "\n",
    "        sdc_edge_indices = []\n",
    "        sdc_edge_attrs = []\n",
    "        sdc_edge_labels = []\n",
    "        cop_edge_indices = []\n",
    "        cop_edge_attrs = []\n",
    "        cop_edge_labels = []\n",
    "        edges = []\n",
    "\n",
    "        file_list = self.raw_file_names\n",
    "        time_steps = len(file_list)\n",
    "        print(time_steps)\n",
    "        train_files, validate_files, test_files = np.split(file_list, [int(.8 * time_steps), int(.9 * time_steps)])\n",
    "\n",
    "        for file_name in tqdm(file_list):\n",
    "            edge_file_path = file_name\n",
    "            # print(edge_file_path)\n",
    "\n",
    "            sdc_edge_index, sdc_edge_attr, sdc_edge_label = self.load_edge_csv(edge_file_path,\n",
    "                                                                               edge_cols=dlvr_edge_attrb + time_attrb,\n",
    "                                                                               src_index_col=\"ship_node_codename\",\n",
    "                                                                               src_mapping=ship_idx_mapping,\n",
    "                                                                               dst_index_col=\"custnode\",\n",
    "                                                                               dst_mapping=cust_idx_mapping,\n",
    "                                                                               encoders={'SLA': IdentityEncoder(\n",
    "                                                                                   dtype=torch.long)})\n",
    "            edges.append(len(sdc_edge_label))\n",
    "\n",
    "            cop_edge_index, cop_edge_attr, cop_edge_label = self.load_edge_csv(edge_file_path,\n",
    "                                                                               edge_cols=prod_edge_attrb,\n",
    "                                                                               src_index_col=\"custnode\",\n",
    "                                                                               src_mapping=cust_idx_mapping,\n",
    "                                                                               dst_index_col=\"prmry_sku_id\",\n",
    "                                                                               dst_mapping=prod_idx_mapping)\n",
    "            # encoders={'SLA': IdentityEncoder(dtype=torch.long)})\n",
    "            sdc_edge_indices.append(sdc_edge_index)\n",
    "            sdc_edge_attrs.append(sdc_edge_attr)\n",
    "            sdc_edge_labels.append(sdc_edge_label)\n",
    "            cop_edge_indices.append(cop_edge_index)\n",
    "            cop_edge_attrs.append(cop_edge_attr)\n",
    "            cop_edge_labels.append(cop_edge_label)\n",
    "\n",
    "        return sdc_edge_indices, sdc_edge_attrs, sdc_edge_labels, cop_edge_indices, cop_edge_attrs, cop_edge_labels\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return 'wmt-network()'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce807a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut\n",
    "root_dir = os.path.join(walk_up_folder(os.getcwd(), 3), \"dataset/root/\")\n",
    "data = WmtNetworkDataset(root_dir)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dff97d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mproduct\u001b[0m={\n",
       "    x=[231219, 759],\n",
       "    num_nodes=231219\n",
       "  },\n",
       "  \u001b[1mcustomer\u001b[0m={\n",
       "    x=[754603, 66],\n",
       "    num_nodes=754603\n",
       "  },\n",
       "  \u001b[1mshipnode\u001b[0m={\n",
       "    x=[9818, 68],\n",
       "    num_nodes=9818\n",
       "  },\n",
       "  \u001b[1m(customer, orders, product)\u001b[0m={\n",
       "    edge_index=[2, 1278579],\n",
       "    edge_attr=[1278579, 4],\n",
       "    edge_label=[1278579, 1]\n",
       "  },\n",
       "  \u001b[1m(shipnode, delivers, customer)\u001b[0m={\n",
       "    edge_index=[2, 1278579],\n",
       "    edge_attr=[1278579, 18],\n",
       "    edge_label=[1278579, 1]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30bb2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def get_edge_split(data):\n",
    "    edge_types = data.edge_types\n",
    "    data = T.ToUndirected()(data)\n",
    "    data = T.AddSelfLoops()(data)\n",
    "    data = T.NormalizeFeatures()(data)\n",
    "    rev_edge_types = [e for e in data.edge_types if e not in edge_types]\n",
    "\n",
    "    for e in rev_edge_types:\n",
    "        del data[e].edge_label\n",
    "    #\n",
    "    # data[\"edge_label\"] = data[('shipnode', 'delivers', 'customer')].edge_label\n",
    "    # del data[('shipnode', 'delivers', 'customer')].edge_label\n",
    "    # Make a copy\n",
    "    train_data, val_data, test_data = copy(data), copy(data), copy(data)\n",
    "    # Create an array based on total data size\n",
    "    num_edges = data[('shipnode', 'delivers', 'customer')].num_edges\n",
    "    arr = torch.arange(num_edges)\n",
    "    # Train test split of 80:10:10\n",
    "    train_idx = arr[:int(0.8 * num_edges)]\n",
    "    val_idx = arr[int(0.8 * num_edges):int(0.9 * num_edges)]\n",
    "    test_idx = arr[int(0.9 * num_edges):]\n",
    "\n",
    "    # Create indexing masks\n",
    "    train_mask = torch.LongTensor(train_idx)\n",
    "    val_mask = torch.LongTensor(val_idx)\n",
    "    test_mask = torch.LongTensor(test_idx)\n",
    "\n",
    "    # edge indexing\n",
    "    for edge in edge_types+rev_edge_types:\n",
    "        train_data[edge].edge_index = train_data[edge].edge_index[:, train_mask]\n",
    "        train_data[edge].edge_attr = train_data[edge].edge_attr[train_mask, :]\n",
    "\n",
    "        val_data[edge].edge_index = val_data[edge].edge_index[:, val_mask]\n",
    "        val_data[edge].edge_attr = val_data[edge].edge_attr[val_mask, :]\n",
    "\n",
    "        test_data[edge].edge_index = test_data[edge].edge_index[:, test_mask]\n",
    "        test_data[edge].edge_attr = test_data[edge].edge_attr[test_mask, :]\n",
    "\n",
    "    # Split labels\n",
    "    for label in data.edge_label_dict:\n",
    "        train_data[label].edge_label = train_data[label].edge_label[train_mask]\n",
    "        val_data[label].edge_label = val_data[label].edge_label[val_mask]\n",
    "        test_data[label].edge_label = test_data[label].edge_label[test_mask]\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02d62d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mproduct\u001b[0m={\n",
       "    x=[231219, 759],\n",
       "    num_nodes=231219\n",
       "  },\n",
       "  \u001b[1mcustomer\u001b[0m={\n",
       "    x=[754603, 66],\n",
       "    num_nodes=754603\n",
       "  },\n",
       "  \u001b[1mshipnode\u001b[0m={\n",
       "    x=[9818, 68],\n",
       "    num_nodes=9818\n",
       "  },\n",
       "  \u001b[1m(customer, orders, product)\u001b[0m={\n",
       "    edge_index=[2, 1022863],\n",
       "    edge_attr=[1022863, 4],\n",
       "    edge_label=[1022863, 1]\n",
       "  },\n",
       "  \u001b[1m(shipnode, delivers, customer)\u001b[0m={\n",
       "    edge_index=[2, 1022863],\n",
       "    edge_attr=[1022863, 18],\n",
       "    edge_label=[1022863, 1]\n",
       "  },\n",
       "  \u001b[1m(product, rev_orders, customer)\u001b[0m={\n",
       "    edge_index=[2, 1022863],\n",
       "    edge_attr=[1022863, 4]\n",
       "  },\n",
       "  \u001b[1m(customer, rev_delivers, shipnode)\u001b[0m={\n",
       "    edge_index=[2, 1022863],\n",
       "    edge_attr=[1022863, 18]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = get_edge_split(data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4898bdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(\"customer\", \"orders\", \"product\")].edge_label.flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20924a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame({'netqty':data[(\"customer\", \"orders\", \"product\")].edge_label.flatten().numpy()})\n",
    "Q1 = labels['netqty'].quantile(0.25)\n",
    "Q3 = labels['netqty'].quantile(0.75)\n",
    "    \n",
    "IQR = Q3 - Q1\n",
    "\n",
    "\n",
    "lower_range = Q1 - 1.5 * IQR\n",
    "upper_range = Q3 + 1.5 * IQR\n",
    "\n",
    "lower_range,upper_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "deacec75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels[(labels['netqty']<0) | (labels['netqty']>15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d417ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data,Batch,LightningNodeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f74a0197",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/dbdj1k317kb6_4pjcgr8j7tr0000gq/T/ipykernel_82362/2026853887.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "LightningNodeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c94d0624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repository.cache.walmart.com/repository/pypi-proxy/simple/, https://pypi.org/simple\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pytorch-lightning/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pytorch-lightning/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pytorch-lightning/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pytorch-lightning/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pytorch-lightning/\u001b[0m\n",
      "Collecting pytorch_lightning\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/pytorch-lightning/pytorch_lightning-1.5.10-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzE4L2YxL2Y1OWIzMDdmNzVkYjE4ODZjOTZlMzk2ZWVjODc4NTAxNTEwNjc3Mzk0ODY4NjgwYjhkMmI4YjU4YzQ3Yy9weXRvcmNoX2xpZ2h0bmluZy0xLjUuMTAtcHkzLW5vbmUtYW55LndobCNzaGEyNTY9Yzg2ZWQ2MzNjZThkMjU2Njc3OTc2MzVkYjNlYTFmZTlmZTZiNTExMjFkNDNkNzAzMWQ1YmRiNGM1YjA0Njg5NQ== (527 kB)\n",
      "\u001b[K     || 527 kB 12 kB/s eta 0:00:011\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/setuptools/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/setuptools/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/setuptools/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/setuptools/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/setuptools/\u001b[0m\n",
      "\u001b[?25hCollecting setuptools==59.5.0\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/setuptools/setuptools-59.5.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzQwL2E5LzdkZWFjNzZjNThmYTQ3Yzk1MzYwMTE2YTA2YjUzYjliNjJmNmRiMTEzMzZmZTYxYjZhYjUzNzg0ZDk4Yi9zZXR1cHRvb2xzLTU5LjUuMC1weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj02ZDEwNzQxZmYyMGI4OWNkOGM2YTUzNmVlOWRjOTBkMzAwMmRlYzAyMjZjNzhmYjk4NjA1YmZiOWVmOGE3YWRm (952 kB)\n",
      "\u001b[K     || 952 kB 186 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pytorch_lightning) (21.0)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard/\u001b[0m\n",
      "Collecting tensorboard>=2.2.0\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/tensorboard/tensorboard-2.8.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Y3L2ZkLzY3YzYxMjc2ZGUwMjU4MDFjZmE4YTFiOWFmMmQ3YzU3N2U3ZjI3YzE3YjZiZmYyYmFjYTIwYmYwMzU0My90ZW5zb3Jib2FyZC0yLjguMC1weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj02NWEzMzhlNDQyNGU5MDc5ZjI2MDQ5MjNiZGJlMzAxNzkyYWRjZTJhY2UxYmU2OGRhNmIzZGRmMDA1MTcwZGVm (5.8 MB)\n",
      "\u001b[K     || 5.8 MB 603 kB/s eta 0:00:01\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pydeprecate/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pydeprecate/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pydeprecate/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pydeprecate/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/pydeprecate/\u001b[0m\n",
      "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/pydeprecate/pyDeprecate-0.3.1-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2EyLzE3L2ZmN2VjMjc1MmY1M2VhMjQ1NDk5YjIzZWU2NGU3NmQxMmY0NWZjZGU3YTViMWI0NDVmOWM1OGNkMWVjMC9weURlcHJlY2F0ZS0wLjMuMS1weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj1iNWRkOGM0YzA1MzU4NTRiNmE1MjkzNmQxMjU2ODgzYTk0MGUzYjAyMDA2ZmM3MTE4YjUzMDI3YzBhY2RlMTgx (10 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pytorch_lightning) (3.10.0.2)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/torchmetrics/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/torchmetrics/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/torchmetrics/\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/torchmetrics/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/torchmetrics/\u001b[0m\n",
      "Collecting torchmetrics>=0.4.1\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/torchmetrics/torchmetrics-0.7.2-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Y3L2VjLzMxNjBmZDJkMzBiNTViMzVlOWNmZDg2NzBjOTVmY2FlYjFkYWE5ZGJhMjhhYTkxMmNmZTQwZDY5NmEzYi90b3JjaG1ldHJpY3MtMC43LjItcHkzLW5vbmUtYW55LndobCNzaGEyNTY9ZDBmYmY4NDQwOTEyZWY5M2YyMmUyMWJhZTQzZmRhOGZhMjZhNjUxMzEzYWNjM2VhOTNiZWFmZTNjODZkZDQ3NA== (397 kB)\n",
      "\u001b[K     || 397 kB 34 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pytorch_lightning) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.7.* in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pytorch_lightning) (1.9.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pytorch_lightning) (2021.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pytorch_lightning) (1.20.3)\n",
      "Requirement already satisfied: aiohttp in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: requests in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/absl-py/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/absl-py/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/absl-py/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/absl-py/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/absl-py/\u001b[0m\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/absl-py/absl_py-1.0.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzJjLzAzL2UzZTE5ZDNmYWY0MzBlZGUzMmU0MTIyMWIyOTRlMzc5NTJlMDZhY2M5Njc4MWM0MTdhYzI1ZDRhMDMyNC9hYnNsX3B5LTEuMC4wLXB5My1ub25lLWFueS53aGwjc2hhMjU2PTg0ZTZkY2RjNjljOTQ3ZDBjMTNlNTQ1N2QwNTZiZDQzY2FkZTRjMjM5M2RjZTAwZDY4NGFlZGVhNzdkZGMyYTM= (126 kB)\n",
      "\u001b[K     || 126 kB 8.0 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.18.1)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-plugin-wit/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-plugin-wit/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-plugin-wit/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-plugin-wit/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-plugin-wit/\u001b[0m\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/tensorboard-plugin-wit/tensorboard_plugin_wit-1.8.1-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2UwLzY4L2U4ZWNmYWM1ZGQ1OTRiNjc2YzIzYTdmMDdlYTM0YzE5N2Q3ZDY5YjMzMTNhZmRmOGFjMWIwYTk5MDVhMi90ZW5zb3Jib2FyZF9wbHVnaW5fd2l0LTEuOC4xLXB5My1ub25lLWFueS53aGwjc2hhMjU2PWZmMjZiZGQ1ODNkMTU1YWE5NTFlZTNiMTUyYjNkMGNmZmFlODAwNWRjNjk3ZjcyYjQ0YThlOGMyYTc3YThjYmU= (781 kB)\n",
      "\u001b[K     || 781 kB 24 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.24.3 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.41.0)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/werkzeug/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/werkzeug/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/werkzeug/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/werkzeug/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/werkzeug/\u001b[0m\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/werkzeug/Werkzeug-2.0.3-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Y0L2YzLzIyYWZiZGIyMGNjNDY1NGIxMGM5ODA0MzQxNGExNDA1N2NkMjdmZGJhOWQ0YWU2MWNlYTU5NjAwMGJhMi9XZXJremV1Zy0yLjAuMy1weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj0xNDIxZWJmYzc2NDhhMzlhNWM1OGM2MDFiMTU0MTY1ZDA1Y2Y0N2EzY2QwY2NiNzA4NTdjYmRhY2Y2YzhmMmI4 (289 kB)\n",
      "\u001b[K     || 289 kB 31 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.3.0)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-data-server/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-data-server/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-data-server/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-data-server/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/tensorboard-data-server/\u001b[0m\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/tensorboard-data-server/tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzNlLzQ4L2RkMTM1ZGJiM2NmMTZiZmI5MjM3MjAxNjM0OTNjYWI3MGU3MzM2ZGI0YjVmMzEwM2Q0OWVmYTczMDQwNC90ZW5zb3Jib2FyZF9kYXRhX3NlcnZlci0wLjYuMS1weTMtbm9uZS1tYWNvc3hfMTBfOV94ODZfNjQud2hsI3NoYTI1Nj1mYThjZWY5YmU0ZmNhZTJmMjM2M2M4ODE3NjYzOGJhZjJkYTE5YzVlYzkwYWRkYjQ5YjFjZGUwNWM5NWM4OGVl (3.5 MB)\n",
      "\u001b[K     || 3.5 MB 54 kB/s eta 0:00:0155\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/markdown/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/markdown/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/markdown/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/markdown/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /simple/markdown/\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/markdown/Markdown-3.3.6-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzlmL2Q0LzJjN2Y4MzkxNWQ0Mzc3MzY5OTZiMjY3NDMwMGM2YzRiNTc4YTZmODk3ZjM0ZTQwZjVjMDRkYjE0NjcxOS9NYXJrZG93bi0zLjMuNi1weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj05OTIzMzMyMzE4Zjg0MzQxMWU5OTMyMjM3NTMwZGY1MzE2MmUyOWRjN2E0ZTJiOTFlMzU3NjQ1ODNjNDZjOWEz (97 kB)\n",
      "\u001b[K     || 97 kB 31 kB/s eta 0:00:0101\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/n0s011m/.conda/envs/mobius-data-science/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
      "Installing collected packages: setuptools, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, pyDeprecate, markdown, absl-py, torchmetrics, tensorboard, pytorch-lightning\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 58.0.4\n",
      "    Uninstalling setuptools-58.0.4:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled setuptools-58.0.4\n",
      "Successfully installed absl-py-1.0.0 markdown-3.3.6 pyDeprecate-0.3.1 pytorch-lightning-1.5.10 setuptools-59.5.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.7.2 werkzeug-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dfacb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(train_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22253c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collate_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/dbdj1k317kb6_4pjcgr8j7tr0000gq/T/ipykernel_82362/2063166536.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'collate_fn' is not defined"
     ]
    }
   ],
   "source": [
    "for indices in loader.batch_sampler:\n",
    "    collate_fn([train_data[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42d4825e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/dbdj1k317kb6_4pjcgr8j7tr0000gq/T/ipykernel_82362/174258208.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msampled_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mobius-data-science/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mobius-data-science/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mobius-data-science/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mobius-data-science/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mobius-data-science/lib/python3.7/site-packages/torch_geometric/data/hetero_data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# If neither is present, we create a new `Storage` object for the given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# node/edge-type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_canonical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mobius-data-science/lib/python3.7/site-packages/torch_geometric/data/hetero_data.py\u001b[0m in \u001b[0;36m_to_canonical\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;31m# Try to find the unique source/destination node tuple:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             edge_types = [\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "sampled_data = next(iter(loader))\n",
    "sampled_data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8e5914284121da5e33e234f24e4d02bdc69909f776a004ca07e9057a3900b7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('networks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
